{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1 - mkecera3@gatech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and save the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start of citation - the following code was addapted from https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=False, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=False, train=False, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(model, valloader, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    for images,labels in valloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        with torch.no_grad():\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "#         print(images.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 784)\n",
    "#             print(img.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model(img)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader)))\n",
    "    \n",
    "    return running_loss/len(valloader), correct_count, all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "    time0 = time()\n",
    "    epochs = 30\n",
    "    lossData = []\n",
    "    accData = []\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "        \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "            \n",
    "        testingLoss, correctCount, allCount = nnPredict(model, valloader, criterion)\n",
    "        print(\"Testing Loss  =\", (testingLoss))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader), 'Training Error'])\n",
    "        lossData.append([e, testingLoss, 'Testing Error'])\n",
    "        accData.append([e, correctCount / allCount])\n",
    "    \n",
    "    return lossData, accData    \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline, accDataBaseline = trainNN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataBaseline = [[row[0], row[1], 'Baseline Accuracy'] for row in accDataBaseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline = pd.DataFrame.from_records(lossDataBaseline, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataBaseline = pd.DataFrame.from_records(accDataBaseline, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataBaseline\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataBaseline\n",
    "    ).set_title('3 hidden NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [256, 128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model_complex = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[2], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model_complex(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataComplex, accDataComplex = trainNN(model_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataComplex = [[row[0], row[1], 'More nodes and layers Accuracy'] for row in accDataComplex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataComplex = pd.DataFrame.from_records(lossDataComplex, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataComplex = pd.DataFrame.from_records(accDataComplex, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataComplex\n",
    "    ).set_title('3 hidden NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataComplex\n",
    "    ).set_title('3 hidden NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "End of citation - the preceding code was addapted from https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model_complex, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [256, 128, 64]\n",
    "output_size = 10\n",
    "drop_out = 0.2\n",
    "\n",
    "model_dropout = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[2], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model_dropout(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataDropout, accDataDropout = trainNN(model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataDropout = [[row[0], row[1], 'Dropout Accuracy'] for row in accDataDropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataDropout = pd.DataFrame.from_records(lossDataDropout, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataDropout = pd.DataFrame.from_records(accDataDropout, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize train and test error across iterations\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataDropout\n",
    "    ).set_title('Dropout NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataDropout\n",
    "    ).set_title('Dropout NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accData = accDataBaseline.copy()\n",
    "accData = accData.append(accDataComplex)\n",
    "accData = accData.append(accDataDropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accData\n",
    "    ).set_title('Accuracy of NN versions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount = nnPredict(model_complex, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount / allCount"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Credit default dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "creditDataDf = pd.read_excel('./data/default of credit card clients.xls', header=1)\n",
    "creditDataDf = creditDataDf.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(creditDataDf['default payment next month']))\n",
    "print(len(creditDataDf))\n",
    "print(sum(creditDataDf['default payment next month'])/len(creditDataDf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables from categorical\n",
    "creditDataDf = pd.get_dummies(creditDataDf, prefix=['SEX', 'EDUCATION', 'MARRIAGE'], columns=['SEX', 'EDUCATION', 'MARRIAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last mummy variable\n",
    "creditDataDf = creditDataDf.drop(columns=['SEX_2', 'EDUCATION_6', 'MARRIAGE_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = creditDataDf['default payment next month']\n",
    "creditDataDf = creditDataDf.drop(columns=['default payment next month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "ros = RandomOverSampler(random_state=37)\n",
    "creditDataDf, y = ros.fit_resample(creditDataDf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "x = creditDataDf.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "creditDataDf = pd.DataFrame(x_scaled, columns=creditDataDf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditDataDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(creditDataDf, y, test_size=0.2, random_state=37)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)\n",
    "print(len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(np.float32))\n",
    "X_val_tensor = torch.tensor(X_val.values.astype(np.float32))\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.int))\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.int))\n",
    "y_val_tensor = torch.tensor(y_val.values.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTorchDataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valTorchDataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "testTorchDataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valTorchDataset), len(testTorchDataset), len(trainTorchDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader2 = torch.utils.data.DataLoader(trainTorchDataset, batch_size=64, shuffle=True)\n",
    "valloader2 = torch.utils.data.DataLoader(valTorchDataset, batch_size=len(valTorchDataset), shuffle=True)\n",
    "testloader2 = torch.utils.data.DataLoader(testTorchDataset, batch_size=len(testTorchDataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredictTabular(model2, valloader2, criterion):\n",
    "    correct_count, all_count = 0, 0\n",
    "    running_loss = 0\n",
    "    probList = []\n",
    "    for rows,labels in valloader2:\n",
    "        \n",
    "#         print(rows.shape)\n",
    "#         print(labels.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                output = model2(rows)\n",
    "                loss = criterion(output, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            row = rows[i].view(1, 30)\n",
    "#             print(row.shape)\n",
    "            with torch.no_grad():\n",
    "                logps = model2(row)\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            probList.append(probab)\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == pred_label):\n",
    "                correct_count += 1\n",
    "            all_count += 1\n",
    "\n",
    "#     print(\"Number Of Images Tested =\", all_count)\n",
    "#     print(\"Model Accuracy =\", (correct_count/all_count))\n",
    "#     print(\"Testing Loss  =\", (running_loss/len(valloader2)))\n",
    "    \n",
    "    return running_loss/len(valloader2), correct_count, all_count, probList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30\n",
    "hidden_sizes = [25, 15]\n",
    "output_size = 2\n",
    "\n",
    "modelTabBaseline = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTabNN(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    time0 = time()\n",
    "    epochs = 100\n",
    "    lossData = []\n",
    "    accData=[]\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for rows, labels in trainloader2:\n",
    "                \n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(rows)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "            \n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader2)))\n",
    "            \n",
    "        testingLoss, correctCount, allCount, proba = nnPredictTabular(model, valloader2, criterion)\n",
    "        print(\"Testing Loss  =\", (testingLoss))\n",
    "        print(\"Testing Accuracy  =\", (correctCount / allCount))\n",
    "        \n",
    "        lossData.append([e, running_loss/len(trainloader2), 'Training Loss'])\n",
    "        lossData.append([e, testingLoss, 'Testing Loss'])\n",
    "        accData.append([e, correctCount / allCount])\n",
    "    \n",
    "    return lossData, accData\n",
    "        \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataBaseline, accDataBaseline = trainTabNN(modelTabBaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount, proba = nnPredictTabular(modelTabBaseline, testloader2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount/allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataBaseline = [[row[0], row[1], 'Baseline Accuracy'] for row in accDataBaseline]\n",
    "lossDataBaseline = pd.DataFrame.from_records(lossDataBaseline, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataBaseline = pd.DataFrame.from_records(accDataBaseline, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataBaseline\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "    \n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataBaseline\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30\n",
    "hidden_sizes = [25, 20, 20, 20, 15, 15]\n",
    "output_size = 2\n",
    "\n",
    "modelTabLarge = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[3], hidden_sizes[4]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[5], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataLarge, accDataLarge = trainTabNN(modelTabLarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount, proba = nnPredictTabular(modelTabLarge, testloader2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount/allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataLarge = [[row[0], row[1], '5 hidden Accuracy'] for row in accDataLarge]\n",
    "lossDataLarge = pd.DataFrame.from_records(lossDataLarge, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataLarge = pd.DataFrame.from_records(accDataLarge, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataLarge\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "    \n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataLarge\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30\n",
    "hidden_sizes = [25, 20, 20, 20, 15, 15]\n",
    "output_size = 2\n",
    "drop_out = 0.2\n",
    "\n",
    "modelTabDropout = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_out, inplace=True),\n",
    "    nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[3], hidden_sizes[4]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[4], hidden_sizes[5]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[5], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDataLarge, accDataLarge = trainTabNN(modelTabDropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLoss, correctCount, allCount, proba = nnPredictTabular(modelTabDropout, testloader2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCount/allCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataLarge = [[row[0], row[1], 'Dropout Accuracy'] for row in accDataLarge]\n",
    "lossDataLarge = pd.DataFrame.from_records(lossDataLarge, columns=[\"Epoch\", \"Loss\", \"Stage\"])\n",
    "accDataLarge = pd.DataFrame.from_records(accDataLarge, columns=[\"Epoch\", \"Accuracy\", \"Stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Loss\",\n",
    "    hue=\"Stage\",\n",
    "    data=lossDataLarge\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "    \n",
    "sns.lineplot(\n",
    "    x=\"Epoch\", y=\"Accuracy\",\n",
    "    hue=\"Stage\",\n",
    "    data=accDataLarge\n",
    "    ).set_title('Baseline NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('./data', download=False, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=False, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_MNIST = trainset.data.numpy()\n",
    "X_test_MNIST = testset.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_MNIST = trainset.targets.numpy()\n",
    "y_test_MNIST = testset.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train_MNIST) + np.bincount(y_test_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_MNIST.shape)\n",
    "print(y_train_MNIST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_MNIST_reshaped = X_train_MNIST.reshape((60000, 784))\n",
    "X_test_MNIST_reshaped = X_test_MNIST.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': np.arange(3, 30, 3), 'min_samples_leaf': np.arange(5, 50, 5) }\n",
    "gridSearchTree = GridSearchCV(\n",
    "    tree.DecisionTreeClassifier(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "gridSearchTree.fit(X_train_MNIST_reshaped, y_train_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearchTree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gridSearchTree.predict(X_test_MNIST_reshaped)\n",
    "print(accuracy_score(preds, y_test_MNIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(gridSearchTree.cv_results_)\n",
    "cvResultsScore = cvResultsScore[['param_max_depth', 'param_min_samples_leaf', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis').set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(gridSearchTree.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_max_depth', 'param_min_samples_leaf', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis').set_title('Training time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(gridSearchTree.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_max_depth', 'param_min_samples_leaf', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 3)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis').set_title('Score time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = tree.DecisionTreeClassifier(**gridSearchTree.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train_MNIST_reshaped,\n",
    "                                            y = y_train_MNIST, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Credit default dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train) + np.bincount(y_test) + np.bincount(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dfs\n",
    "X_train = pd.concat([X_train,X_val], axis=0)\n",
    "y_train = pd.concat([y_train,y_val], axis=0)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': np.arange(3, 30, 3), 'min_samples_leaf': np.arange(5, 50, 5) }\n",
    "gridSearchTree = GridSearchCV(\n",
    "    tree.DecisionTreeClassifier(), param_grid, n_jobs=-1, verbose=2, scoring='accuracy'\n",
    "    )\n",
    "gridSearchTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gridSearchTree.predict(X_test)\n",
    "probs = gridSearchTree.predict_proba(X_test)\n",
    "print(accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(gridSearchTree.cv_results_)\n",
    "cvResultsScore = cvResultsScore[['param_max_depth', 'param_min_samples_leaf', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis').set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(gridSearchTree.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_max_depth', 'param_min_samples_leaf', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis').set_title('Training time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(gridSearchTree.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_max_depth', 'param_min_samples_leaf', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 3)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis').set_title('Score time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = tree.DecisionTreeClassifier(**gridSearchTree.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train,\n",
    "                                            y = y_train, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': np.arange(3, 30, 3), 'n_estimators': np.arange(10, 100, 10) }\n",
    "gridSearchBoosting = GridSearchCV(\n",
    "    GradientBoostingClassifier(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "gridSearchBoosting.fit(X_train_MNIST_reshaped, y_train_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearchBoosting.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gridSearchBoosting.predict(X_test_MNIST_reshaped)\n",
    "print(accuracy_score(preds, y_test_MNIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(gridSearchBoosting.cv_results_)\n",
    "cvResultsScore = cvResults[['param_max_depth', 'param_n_estimators', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_max_depth', columns='param_n_estimators', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(gridSearchBoosting.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_max_depth', 'param_n_estimators', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis').set_title('Training time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(gridSearchBoosting.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_max_depth', 'param_n_estimators', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 3)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis').set_title('Score time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = GradientBoostingClassifier(**gridSearchBoosting.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train_MNIST_reshaped,\n",
    "                                            y = y_train_MNIST, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Credit default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': np.arange(3, 30, 3), 'n_estimators': np.arange(10, 100, 10) }\n",
    "gridSearchBoosting = GridSearchCV(\n",
    "    GradientBoostingClassifier(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "gridSearchBoosting.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gridSearchBoosting.predict(X_test)\n",
    "probs = gridSearchBoosting.predict_proba(X_test)\n",
    "print(accuracy_score(preds, y_test))\n",
    "probs = [row[1] for row in probs]\n",
    "print(gridSearchBoosting.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(gridSearchBoosting.cv_results_)\n",
    "cvResultsScore = cvResults[['param_max_depth', 'param_n_estimators', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_max_depth', columns='param_n_estimators', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(gridSearchBoosting.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_max_depth', 'param_n_estimators', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis').set_title('Training time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(gridSearchBoosting.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_max_depth', 'param_n_estimators', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 3)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis').set_title('Score time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = GradientBoostingClassifier(**gridSearchBoosting.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train,\n",
    "                                            y = y_train, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.arange(3, 24, 3), 'weights': ['uniform', 'distance']}\n",
    "kNN = GridSearchCV(\n",
    "    KNeighborsClassifier(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "kNN.fit(X_train_MNIST_reshaped, y_train_MNIST)\n",
    "\n",
    "preds = kNN.predict(X_test_MNIST_reshaped)\n",
    "print(accuracy_score(preds, y_test_MNIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(kNN.cv_results_)\n",
    "cvResultsScore = cvResultsScore[['param_n_neighbors', 'param_weights', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_n_neighbors', columns='param_weights', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(kNN.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_n_neighbors', 'param_weights', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_n_neighbors', columns='param_weights', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Training Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(kNN.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_n_neighbors', 'param_weights', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 2)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_n_neighbors', columns='param_weights', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Score Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = KNeighborsClassifier(**kNN.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train_MNIST_reshaped,\n",
    "                                            y = y_train_MNIST, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Credit default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.arange(3, 24, 3), 'weights': ['uniform', 'distance']}\n",
    "kNN = GridSearchCV(\n",
    "    KNeighborsClassifier(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = kNN.predict(X_test)\n",
    "probs = kNN.predict_proba(X_test)\n",
    "print(accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(kNN.cv_results_)\n",
    "cvResultsScore = cvResultsScore[['param_n_neighbors', 'param_weights', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_n_neighbors', columns='param_weights', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(kNN.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_n_neighbors', 'param_weights', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_n_neighbors', columns='param_weights', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Training Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(kNN.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_n_neighbors', 'param_weights', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 2)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_n_neighbors', columns='param_weights', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Score Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = KNeighborsClassifier(**kNN.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train,\n",
    "                                            y = y_train, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'kernel': ['poly', 'rbf', 'linear'], 'max_iter': np.arange(10, 100, 10)}\n",
    "SVM = GridSearchCV(\n",
    "    svm.SVC(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "SVM.fit(X_train_MNIST_reshaped, y_train_MNIST)\n",
    "\n",
    "preds = SVM.predict(X_test_MNIST_reshaped)\n",
    "print(accuracy_score(preds, y_test_MNIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(SVM.cv_results_)\n",
    "cvResultsScore = cvResultsScore[['param_kernel', 'param_max_iter', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_kernel', columns='param_max_iter', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(SVM.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_kernel', 'param_max_iter', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_test_score'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_kernel', columns='param_max_iter', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Training Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(SVM.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_kernel', 'param_max_iter', 'mean_fit_time']]\n",
    "cvResultsTimeScore['mean_fit_time'] = round(cvResultsTimeScore['mean_test_score'], 2)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_kernel', columns='param_max_iter', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Score Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = svm.SVC(**SVM.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train_MNIST_reshaped,\n",
    "                                            y = y_train_MNIST, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Credit default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'kernel': ['poly', 'rbf', 'linear'], 'max_iter': np.arange(10, 100, 10)}\n",
    "SVM = GridSearchCV(\n",
    "    svm.SVC(), param_grid, n_jobs=-1, verbose=3, scoring='accuracy'\n",
    "    )\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "preds = SVM.predict(X_test)\n",
    "print(accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsScore = pd.DataFrame(SVM.cv_results_)\n",
    "cvResultsScore = cvResultsScore[['param_kernel', 'param_max_iter', 'mean_test_score']]\n",
    "cvResultsScore['mean_test_score'] = round(cvResultsScore['mean_test_score'], 2)\n",
    "cvResultsScore = cvResultsScore.pivot(index='param_kernel', columns='param_max_iter', values='mean_test_score')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeTrain = pd.DataFrame(SVM.cv_results_)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain[['param_kernel', 'param_max_iter', 'mean_fit_time']]\n",
    "cvResultsTimeTrain['mean_fit_time'] = round(cvResultsTimeTrain['mean_fit_time'], 2)\n",
    "cvResultsTimeTrain = cvResultsTimeTrain.pivot(index='param_kernel', columns='param_max_iter', values='mean_fit_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeTrain, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Training Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResultsTimeScore = pd.DataFrame(SVM.cv_results_)\n",
    "cvResultsTimeScore = cvResultsTimeScore[['param_kernel', 'param_max_iter', 'mean_score_time']]\n",
    "cvResultsTimeScore['mean_score_time'] = round(cvResultsTimeScore['mean_score_time'], 2)\n",
    "cvResultsTimeScore = cvResultsTimeScore.pivot(index='param_kernel', columns='param_max_iter', values='mean_score_time')\n",
    "sns.set_context(\"paper\")\n",
    "sns.heatmap(cvResultsTimeScore, annot=True, fmt=\"g\", cmap='viridis', linewidths=.5).set_title('Score Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = svm.SVC(**SVM.best_params_)\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "                                            estimator = bestModel,\n",
    "                                            X = X_train,\n",
    "                                            y = y_train, \n",
    "                                            train_sizes = np.linspace(0.1, 1.0, 5), \n",
    "                                            scoring = 'accuracy',\n",
    "                                            cv=5\n",
    "                                            )\n",
    "lcurvePlotData = pd.DataFrame({'Train': np.mean(train_scores, axis=1), 'Validation': np.mean(valid_scores, axis=1), 'Train size': train_sizes})\n",
    "lcurvePlotData = lcurvePlotData.melt(id_vars=['Train size'], value_vars=['Train', 'Validation'])\n",
    "lcurvePlotData.rename(columns={'value': 'Accuracy'}, inplace=True)\n",
    "\n",
    "sns.set(\n",
    "    style=\"darkgrid\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    x=\"Train size\", y=\"Accuracy\",\n",
    "    hue=\"variable\",\n",
    "    data=lcurvePlotData\n",
    "    ).set_title('Learning curve - decision tree credit default')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ml': conda)",
   "language": "python",
   "name": "python361064bitmlconda3291cc5c44564875b7e62800a0ae8d21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}